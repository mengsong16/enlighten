# for original pointgoal dataset
dataset_type: PointNav-v1 # for pointgoal dataset
split: val_mini  #[train, val, val_mini] # for pointgoal dataset
dataset_path: "/dataset/pointnav_gibson_v1/{split}/{split}.json.gz" # where pointgoal episode split data store
pointgoal_dataset_path: "/dataset/pointnav_gibson_v1"
content_scenes: ["*"]
scenes_dir: "/dataset"  # where mesh data store
# -----------------------------------------------
# BC training data location
behavior_dataset_path: "/dataset/behavior_dataset_gibson_1_scene_Rancocas_2000_polar_q"  # where imitation learning dataset store


# experiments
algorithm_name: "mlp_sqn"
experiment_name: "rel_goal_1_scene_Rancocas_2000_observation_polar_q_compare_distribution_advantage_mean_q_normalize"
log_to_wandb: true #true

# dummy initial setup 
scene_id: /home/meng/habitat-sim/data/scene_datasets/habitat-test-scenes/van-gogh-room.glb
agent_initial_position: [3, 0, 1]
agent_initial_rotation: [0, 70, 0]
goal_position: [4, 0, -1]

# dummy for saving render images
eval_video_option: [] #["disk", "tensorboard"]

# robot
forward_resolution: 0.25 # meter
rotate_resolution: 10 # degree
action_number: 4  # cartesian action space

# observation space (env)
state_sensor: true
color_sensor: true
depth_sensor: false #true
semantic_sensor: false
image_width: 224 #256
image_height: 224 #256 
normalize_depth: true # if true, scale depth to [0,1] according to min and max depth
min_depth: 0
max_depth: 10.0 
dictionary_observation_space: true

# task
measurements: ['steps', 'collisions', 'distance_to_goal', 'success', 'done', 'spl', 'softspl', 'point_goal_reward', 'return']

# state
state_coord_system: polar # [polar, cartesian]
state_dimension: 2 # [2, 3]
cos_augmented_state: false #false
state_relative_to_origin: true
state_only: false # only use state as input to the agent

# goal
goal_conditioned: true  # enable goal sensor
goal_format: pointgoal # imagegoal #pointgoal
goal_relative_to_origin: false
goal_gps_compass: true #true  # false use absolute goal location relative to the start location
goal_coord_system: polar # [polar, cartesian]
goal_dimension: 2 # [2, 3]
cos_augmented_goal: false #false

# reward
success_distance: 0.2 # l2 distance in meter # must be smaller than movement distance of the forward action
success_reward: 2.5 #100 #2.5 
slack_reward: -1.0e-4 #-1 #-1.0e-4 #-1.0e-2
goal_reward: true # false
stop_depend_success: true

# episode termination condition (environment)
max_steps_per_episode: 500  # used as the longest evaluation steps
max_collisions_per_episode: 200

# seed
seed: 1

# gpu
gpu_id: 1 #1

# vector env (for evaluation only) train 10, evaluate 14
num_environments: 10 #10 #14 #8 #14 #4 #2

# q network
hidden_size: 512 #1024
hidden_layer: 1 # 1 when state_form is observation, 2 when state_form is state
obs_embedding_size: 512
goal_embedding_size: 32
goal_form:  "rel_goal" # ["rel_goal", "distance_to_goal", "abs_goal"]
state_form: "observation" # ["state", "observation"]


# batch
batch_mode: "transition"

# training
batch_size: 512 #256
optimizer: "AdamW" #["AdamW", "Adam"]
learning_rate: 1.0e-4 #1.0e-4 #1.0e-3 #5.0e-4
weight_decay: 1.0e-4
max_epochs: 400
gamma: 0.99
#---------------
reward_type: "minus_one_zero" #["original", "minus_one_zero"]
negative_reward_scale: 1 #1.0e-4 # default is 1
positive_reward: 0 #2.5 # dafault is 0
action_type: "polar" #["cartesian", "polar"]
loss_function: "compare_distribution" #["compare_value", "compare_distribution"]
supervise_advantage: true # supervise advantage or q
value_function_type: "mean_q" #["mean_q", "max_q"]
#---------------
policy_type: "boltzmann" # ["max_q", "boltzmann"]
greedy_policy: false #true
prob_convert_method: "normalize" #["softmax", "normalize"]
temperature: 1 #0.01
#---------------

# evaluate during training
eval_during_training: true #false
eval_every_epochs: 10 #10  
eval_during_training_sample: false # dummy
eval_during_training_splits: ["same_start_goal_val_mini"] #["same_start_goal_val_mini", "same_scene_val_mini", "across_scene_val_mini"]
eval_use_vector_envs: true #true

# save checkpoint
save_every_epochs: 10

# evaluation
eval_experiment_folder: "mlp_sqn-rel_goal_1_scene_rancocas_2000_observation_polar_q_compare_value_advantage_max_q-s1-20221107-133951"
eval_checkpoint_file: ["ckpt_13.pth", "ckpt_16.pth", "ckpt_25.pth", "ckpt_32.pth", "ckpt_37.pth"] 
eval_dir: "evaluation"













