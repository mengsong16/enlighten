# for online sac dataset
behavior_dataset_path: "/dataset/behavior_dataset_gibson_1_scene_Rancocas_2000" 

# dummy initial setup 
scene_id: /home/meng/habitat-sim/data/scene_datasets/habitat-test-scenes/van-gogh-room.glb
agent_initial_position: [3, 0, 1]
agent_initial_rotation: [0, 70, 0]
goal_position: [4, 0, -1]

# robot
forward_resolution: 0.25 # meter
rotate_resolution: 10 # degree
action_number: 4 #[4,5,6]

# observation space (env)
state_sensor: true
color_sensor: true
depth_sensor: false #true
semantic_sensor: false
image_width: 224 #256
image_height: 224 #256 
normalize_depth: true # if true, scale depth to [0,1] according to min and max depth
min_depth: 0
max_depth: 10.0 #5.0
dictionary_observation_space: true

# task
dark_mode: false #true #false
flashlight_z: 0.2 #0.8 #0.2
measurements: ['steps', 'collisions', 'distance_to_goal', 'success', 'done', 'spl', 'softspl', 'point_goal_reward', 'return']
blind_agent: false

# state
state_coord_system: polar # [polar, cartesian]
state_dimension: 2 # [2, 3]
cos_augmented_state: false #true #false
state_relative_to_origin: true
state_only: false #false # true when only use coordinates as input to the agent

# goal
goal_conditioned: true #false
goal_format: pointgoal # imagegoal #pointgoal
goal_relative_to_origin: false #false
goal_gps_compass: true #true  # false use absolute goal location relative to the start location
goal_coord_system: polar # [polar, cartesian]
goal_dimension: 2 # [2, 3]
cos_augmented_goal: false #true #false

# reward
success_distance: 0.2 # l2 distance in meter
success_reward: 2.5 #2.5 
slack_reward: -1.0e-4 #-1 #-1.0e-4 #-1.0e-2
goal_reward: true #true # false
intrinsic_reward_coef: 0 #0.1 # >=0
prev_state_novelty_coef: 0.5 #0.5
depth_reward: false
stop_depend_success: true #true

# episode termination condition
max_steps_per_episode: 500 
max_collisions_per_episode: 200 

# seed and device
gpu_id: 1
seed: 1

# data collection
num_steps: 128  # rollout buffer length
# vector env
num_environments: 6 #6 #4 #14 #8
# replay buffer
max_replay_buffer_size: 10000
sample_with_replace: true

# neural network
visual_encoder: ResNet #ResNet # CNN 
obs_embedding_size: 512
goal_embedding_size: 32
hidden_size: 512
hidden_layer: 2

# training
num_epochs: 100 #3000
# each epoch has one training loop
# how many updates does one loop have
num_trains_per_train_loop: 100 #1000 
num_expl_steps_per_train_loop: 1000
num_expl_steps_before_training: 1000
batch_size: 256

# save checkpoint
save_every_epochs: 10 

# SAC
discount: 0.99
soft_target_tau: 5.0e-3
target_update_period: 1
encoder_lr: 3.0e-4
policy_lr: 3.0e-4
qf_lr: 3.0e-4

# evaluation (use current config file to evaluate)
eval_dir: "evaluation"  # save to
eval_experiment_folder: "sac-multienv_1_scene_rancocas_2000-s1-20230130-003910" 
eval_checkpoint_file: ["ckpt_4.pth"]

# logs
log_to_wandb: true
algorithm_name: "sac"
experiment_name:  "multienv_1_scene_Rancocas_2000" 
















