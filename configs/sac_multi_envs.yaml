# scene
single_scene: false # train on single scene or a scene dataset

# for original pointgoal dataset
dataset_type: PointNav-v1
split: val_mini  #[train, val, val_mini] # for pointgoal dataset
dataset_path: "/dataset/pointnav_gibson_v1/{split}/{split}.json.gz" # where pointgoal episode split data store
pointgoal_dataset_path: "/dataset/pointnav_gibson_v1"
content_scenes: ["*"]
scenes_dir: "/dataset"  # where mesh data store

# experiments
algorithm_name: "sac"
experiment_name:  "sac_multienv_1_scene_Rancocas_2000" 

# dummy initial setup 
scene_id: /home/meng/habitat-sim/data/scene_datasets/habitat-test-scenes/van-gogh-room.glb
agent_initial_position: [3, 0, 1]
agent_initial_rotation: [0, 70, 0]
goal_position: [4, 0, -1]

# robot
forward_resolution: 0.25 # meter
rotate_resolution: 10 # degree
action_number: 4 #[4,5,6]

# observation space (env)
state_sensor: true
color_sensor: true
depth_sensor: false #true
semantic_sensor: false
image_width: 224 #256
image_height: 224 #256 
normalize_depth: true # if true, scale depth to [0,1] according to min and max depth
min_depth: 0
max_depth: 10.0 #5.0
dictionary_observation_space: true

# task
dark_mode: false #true #false
flashlight_z: 0.2 #0.8 #0.2
measurements: ['steps', 'collisions', 'distance_to_goal', 'success', 'done', 'spl', 'softspl', 'point_goal_reward', 'return']
blind_agent: false

# state
state_coord_system: polar # [polar, cartesian]
state_dimension: 2 # [2, 3]
cos_augmented_state: false #true #false
state_relative_to_origin: true
state_only: false #false # true when only use coordinates as input to the agent

# goal
goal_conditioned: true #false
goal_format: pointgoal # imagegoal #pointgoal
goal_relative_to_origin: false #false
goal_gps_compass: true #true  # false use absolute goal location relative to the start location
goal_coord_system: polar # [polar, cartesian]
goal_dimension: 2 # [2, 3]
cos_augmented_goal: false #true #false

# reward
success_distance: 0.2 # l2 distance in meter
success_reward: 2.5 #2.5 
slack_reward: -1.0e-4 #-1 #-1.0e-4 #-1.0e-2
goal_reward: true #true # false
intrinsic_reward_coef: 0 #0.1 # >=0
prev_state_novelty_coef: 0.5 #0.5
depth_reward: false
stop_depend_success: true #true

# episode termination condition
max_steps_per_episode: 500 
max_collisions_per_episode: 200 

# dummy: to be consistent with imitation learning config 
goal_form:  "rel_goal"
gpu_id: 0

# neural network
visual_encoder: ResNet #ResNet # CNN 


# sampling
num_steps: 128  # rollout buffer length

# vector env (for training and evaluation, train 6, eval 14)
num_environments: 6 #6 #4 #14 #8 #6

# training
torch_gpu_id: 0
simulator_gpu_id: 0
seed: 1

# train for how long: one of them must be -1
# num_updates: how many times agent.update is called, i.e. how many times the training process loops
# num of backprop inside each agent update = ppo_epoch * num_mini_batch
num_updates: -1 #1000
# total_num_steps: steps collected in all environments (num_environments)  
total_num_steps: 5000000 #500000 #300000   # 75e6
# checkpoint
# only one of them can be used
num_checkpoints: 10 #15 #20 #100
checkpoint_interval: -1  
# observation transformations
enabled_transforms: []

# training stats and log
verbose: true # false
log_interval: 25  # log stats every n training updates
reward_window_size: 50 # compute stats every n episodes
tensorboard_dir: "tensorboard"

# SAC
clip_param: 0.2 #0.2 #0.6 # 0.3 no improve
kl_coef: 0
# ppo_epoch: # of training epoches inside each time agent update 
# num of backprop inside each agent update = ppo_epoch * num_mini_batch
ppo_epoch: 4
# batches per each epoch
# each batch include steps from num_environments / num_mini_batch rollouts
# each rollout include num_steps steps
# use one batch for each parameter update 
num_mini_batch: 1 #2  
value_loss_coef: 0.5
entropy_coef: 0.01
lr: 2.5e-4 #2.5e-3 #1e-3 #2.5e-4
eps: 1.0e-5 #1e-5
max_grad_norm: 0.5
# visual encoder output size = state encoder output size = rnn hidden size           
hidden_size: 512  
use_gae: true
gamma: 0.99
tau: 0.95 # 0.98 hurts
use_linear_clip_decay: true
use_linear_lr_decay: true
use_normalized_advantage: false

# evaluation
# save images and video to disk under video_path
# evaluate this folder
eval_checkpoint_folder: "checkpoints"
# if eval_checkpoint_file is an existing file in eval_checkpoint_folder, eval single checkpoint
# otherwise, eval all checkpoints in eval_checkpoint_folder
# if a list, need to ensure that checkpoint index is increasing
eval_checkpoint_file: ["ckpt.9.pth"] #["*"] #["ckpt.8.pth","ckpt.9.pth"] 
# use checkpoint config file
eval_use_ckpt_config: false
# save evaluation results here
eval_dir: "evaluation"
# number of evaluation episodes 
# -1: test on all episodes in the dataset
# number of evaluation episodes in a single scene
test_episode_count: -1 
eval_splits: ["same_scene_val", "across_scene_val", "same_start_goal_val"]
eval_experiment_folder: "pointgoal_baseline_multienv_1_scene_Rancocas_2000_observation_mlp_5m"  #"pointgoal_baseline_multienv_1_scene_Rancocas_whole_5m" "pointgoal_baseline_multienv_4_scene_3m" 
















