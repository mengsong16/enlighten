#!/usr/bin/env python3

# Copyright (c) Facebook, Inc. and its affiliates.
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

from typing import Optional, Tuple

import torch
from torch import Tensor
from torch import nn as nn
from torch import optim as optim

from habitat_sim.utils import profiling_utils
from enlighten.agents.common.rollout_storage import RolloutStorage
from enlighten.agents.models import Policy

import torch.nn.functional as F


class PPO(nn.Module):
    def __init__(
        self,
        actor_critic: Policy,
        clip_param: float,
        ppo_epoch: int,
        num_mini_batch: int,
        value_loss_coef: float,
        entropy_coef: float,
        kl_coef: float,
        lr: Optional[float] = None,
        eps: Optional[float] = None,
        max_grad_norm: Optional[float] = None,
        use_clipped_value_loss: bool = True,
        use_normalized_advantage: bool = True,
    ) -> None:

        super().__init__()

        self.actor_critic = actor_critic

        self.clip_param = clip_param
        self.ppo_epoch = ppo_epoch
        self.num_mini_batch = num_mini_batch

        self.value_loss_coef = value_loss_coef
        self.entropy_coef = entropy_coef
        self.kl_coef = kl_coef

        self.max_grad_norm = max_grad_norm
        self.use_clipped_value_loss = use_clipped_value_loss

        # use Adam as optimizer
        self.optimizer = optim.Adam(
            list(filter(lambda p: p.requires_grad, actor_critic.parameters())),
            lr=lr,
            eps=eps,
        )
        # momentum = 0
        #self.optimizer = torch.optim.SGD(list(filter(lambda p: p.requires_grad, actor_critic.parameters())), lr=lr)

        self.device = next(actor_critic.parameters()).device
        self.use_normalized_advantage = use_normalized_advantage

    def forward(self, *x):
        raise NotImplementedError

    def get_advantages(self, rollouts: RolloutStorage) -> Tensor:
        advantages = (
            rollouts.buffers["returns"][:-1]
            - rollouts.buffers["value_preds"][:-1]
        )
        if not self.use_normalized_advantage:
            return advantages

        EPS_PPO = 1e-5
        return (advantages - advantages.mean()) / (advantages.std() + EPS_PPO)

    def kl(self, log_P, log_Q):
        P = torch.exp(log_P)
        return F.kl_div(log_Q, P, None, None, 'sum')

        
    def update(self, rollouts: RolloutStorage) -> Tuple[float, float, float]:
        advantages = self.get_advantages(rollouts)

        value_loss_epoch = 0.0
        action_loss_epoch = 0.0
        dist_entropy_epoch = 0.0
        kl_divergence_epoch = 0.0

        for _e in range(self.ppo_epoch):
            profiling_utils.range_push("PPO.update epoch")
            # generate training batches
            # each batch include a sequence and h0
            data_generator = rollouts.recurrent_generator(
                advantages, self.num_mini_batch
            )

            #print("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")
            #print(data_generator)
            #print("Start iterating batches")
            #print("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")
            #i = 0
            for batch in data_generator:
                
                #print(batch["observations"].size())
                #print("**********************")
                #print(i)
                # print(batch["recurrent_hidden_states"].size())
                # print(batch["prev_actions"].size())
                # print(batch["actions"].size())
                # print("**********************")
                #exit()

                # evaluate a sequence using new (current) rnn policy, therefore only needs h0 
                # (the rest h were generated by the old recurrent policy)
                # evaluate step by step and stack the results together
                #print("update")
                (
                    values,
                    action_log_probs,
                    dist_entropy,
                    _,
                ) = self._evaluate_actions(
                    batch["observations"],
                    batch["recurrent_hidden_states"], # h0
                    batch["prev_actions"],
                    batch["masks"],
                    batch["actions"],
                )

                
                # action_log_probs: log prob of current pi
                # batch["action_log_probs"]: log prob of old pi
                ratio = torch.exp(action_log_probs - batch["action_log_probs"])
                surr1 = ratio * batch["advantages"]
                surr2 = (
                    torch.clamp(
                        ratio, 1.0 - self.clip_param, 1.0 + self.clip_param
                    )
                    * batch["advantages"]
                )
                action_loss = -(torch.min(surr1, surr2).mean())

                # kl(P=old, Q=new)
                action_kl = self.kl(batch["action_log_probs"], action_log_probs)

                #print("kL: %f"%action_kl)

                if self.use_clipped_value_loss:
                    value_pred_clipped = batch["value_preds"] + (
                        values - batch["value_preds"]
                    ).clamp(-self.clip_param, self.clip_param)
                    value_losses = (values - batch["returns"]).pow(2)
                    value_losses_clipped = (
                        value_pred_clipped - batch["returns"]
                    ).pow(2)
                    value_loss = 0.5 * torch.max(
                        value_losses, value_losses_clipped
                    )
                else:
                    value_loss = 0.5 * (batch["returns"] - values).pow(2)

                value_loss = value_loss.mean()
                dist_entropy = dist_entropy.mean()

                # clear grad on computing graph
                self.optimizer.zero_grad()

                # compute totatl loss
                total_loss = (
                    value_loss * self.value_loss_coef
                    + action_loss
                    - dist_entropy * self.entropy_coef
                    + action_kl * self.kl_coef 
                )

                self.before_backward(total_loss)
                #with torch.autograd.set_detect_anomaly(True):
                total_loss.backward()
                self.after_backward(total_loss)

                self.before_step()
                # update NN weights 
                self.optimizer.step()
                self.after_step()

                value_loss_epoch += value_loss.item()
                action_loss_epoch += action_loss.item()
                dist_entropy_epoch += dist_entropy.item()
                kl_divergence_epoch += action_kl.item()

                #i += 1

            #exit()

            profiling_utils.range_pop()  # PPO.update epoch

        num_updates = self.ppo_epoch * self.num_mini_batch

        value_loss_epoch /= num_updates
        action_loss_epoch /= num_updates
        dist_entropy_epoch /= num_updates
        kl_divergence_epoch /= num_updates

        return value_loss_epoch, action_loss_epoch, dist_entropy_epoch, kl_divergence_epoch

    # rnn_hidden_states: h0 when evaluate a sequence, h_t when evaluate one step
    def _evaluate_actions(
        self, observations, rnn_hidden_states, prev_actions, masks, action
    ):
        r"""Internal method that calls Policy.evaluate_actions.  This is used instead of calling
        that directly so that that call can be overrided with inheritance
        """
        return self.actor_critic.evaluate_actions(
            observations, rnn_hidden_states, prev_actions, masks, action
        )

    def before_backward(self, loss: Tensor) -> None:
        pass

    def after_backward(self, loss: Tensor) -> None:
        pass

    def before_step(self) -> None:
        nn.utils.clip_grad_norm_(
            self.actor_critic.parameters(), self.max_grad_norm
        )

    def after_step(self) -> None:
        pass
